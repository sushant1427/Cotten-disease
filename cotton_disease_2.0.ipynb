{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca250d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1788ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "EPOCHS = 25\n",
    "\n",
    "# Directories\n",
    "train_dir = 'dataset/train'\n",
    "val_dir = 'dataset/val'\n",
    "test_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae524e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1562 images belonging to 4 classes.\n",
      "Found 389 images belonging to 4 classes.\n",
      "Found 18 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8306800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayesh\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.6423 - loss: 2.4126 - val_accuracy: 0.8047 - val_loss: 0.5532\n",
      "Epoch 2/25\n",
      "\u001b[1m 1/48\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 490ms/step - accuracy: 0.9062 - loss: 0.2352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayesh\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9062 - loss: 0.2352 - val_accuracy: 0.8000 - val_loss: 0.5013\n",
      "Epoch 3/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.8853 - loss: 0.3299 - val_accuracy: 0.8333 - val_loss: 0.3898\n",
      "Epoch 4/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0336 - val_accuracy: 1.0000 - val_loss: 0.0680\n",
      "Epoch 5/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.8855 - loss: 0.2865 - val_accuracy: 0.8932 - val_loss: 0.2708\n",
      "Epoch 6/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9062 - loss: 0.2799 - val_accuracy: 1.0000 - val_loss: 0.1475\n",
      "Epoch 7/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.9301 - loss: 0.1861 - val_accuracy: 0.8828 - val_loss: 0.2825\n",
      "Epoch 8/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.3379 - val_accuracy: 0.8000 - val_loss: 0.2056\n",
      "Epoch 9/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - accuracy: 0.9338 - loss: 0.1665 - val_accuracy: 0.9167 - val_loss: 0.2182\n",
      "Epoch 10/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0615 - val_accuracy: 1.0000 - val_loss: 0.0069\n",
      "Epoch 11/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.9359 - loss: 0.1646 - val_accuracy: 0.8490 - val_loss: 0.4562\n",
      "Epoch 12/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.3079 - val_accuracy: 1.0000 - val_loss: 0.0273\n",
      "Epoch 13/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.9621 - loss: 0.1246 - val_accuracy: 0.8125 - val_loss: 0.5084\n",
      "Epoch 14/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2732 - val_accuracy: 0.8000 - val_loss: 0.9622\n",
      "Epoch 15/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9593 - loss: 0.1146 - val_accuracy: 0.7370 - val_loss: 0.8574\n",
      "Epoch 16/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4232 - val_accuracy: 1.0000 - val_loss: 0.0088\n",
      "Epoch 17/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.9424 - loss: 0.1331 - val_accuracy: 0.8854 - val_loss: 0.2734\n",
      "Epoch 18/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0403 - val_accuracy: 1.0000 - val_loss: 0.0180\n",
      "Epoch 19/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9575 - loss: 0.1221 - val_accuracy: 0.9375 - val_loss: 0.1570\n",
      "Epoch 20/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 0.0127\n",
      "Epoch 21/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.9505 - loss: 0.1379 - val_accuracy: 0.9323 - val_loss: 0.1537\n",
      "Epoch 22/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0287 - val_accuracy: 1.0000 - val_loss: 0.0120\n",
      "Epoch 23/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.9546 - loss: 0.1209 - val_accuracy: 0.9427 - val_loss: 0.1581\n",
      "Epoch 24/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.0496 - val_accuracy: 0.8000 - val_loss: 0.6133\n",
      "Epoch 25/25\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9526 - loss: 0.1372 - val_accuracy: 0.9323 - val_loss: 0.1876\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2 model with pretrained weights\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Adding custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freezing the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# # Unfreeze some layers and fine-tune the model\n",
    "# for layer in base_model.layers[:100]:\n",
    "#     layer.trainable = False\n",
    "# for layer in base_model.layers[100:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# # Re-compile the model with a lower learning rate\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Continue training\n",
    "# history_fine = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=val_generator.samples // BATCH_SIZE,\n",
    "#     epochs=EPOCHS // 2\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be98fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8889 - loss: 0.2110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8888888955116272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // BATCH_SIZE)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the model\n",
    "model.save('cotton_disease_model.h5')\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('cotton_disease_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f963735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "The predicted class is: fresh cotton leaf\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    class_idx = np.argmax(prediction, axis=1)[0]\n",
    "    class_labels = ['diseased cotton leaf', 'diseased cotton plant', 'fresh cotton leaf', 'fresh cotton plant']\n",
    "    return class_labels[class_idx]\n",
    "\n",
    "# Example prediction\n",
    "\n",
    "image_path = 'dataset/test/fresh cotton leaf/d (396).jpg'\n",
    "predicted_class = predict_image(image_path)\n",
    "print(f'The predicted class is: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661d88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
